import tensorflow as tf
import os
import numpy as np
import pandas as pd
from time import strftime
from PIL import Image
X_TRAIN_PATH='digit_xtrain.csv'
X_TEST_PATH='digit_xtest.csv'
Y_TRAIN_PATH='digit_ytrain.csv'
Y_TEST_PATH='digit_ytest.csv'
LOGGING_PATH='tensorboard_mnist_digits_log/'

NR_CLASSES=10
VALIDATION_SIZE=10000
IMAGE_WIDTH=28
IMAGE_HEIGHT=28
CHANNELS=1
TOTAL_INPUTS=IMAGE_HEIGHT*IMAGE_WIDTH*CHANNELS
%%time
x_test=np.loadtxt(X_TEST_PATH,delimiter=',',dtype=int)
x_train_all=np.loadtxt(X_TRAIN_PATH,delimiter=',',dtype=int)
y_train_all=np.loadtxt(Y_TRAIN_PATH,delimiter=',',dtype=int)
y_test=np.loadtxt(Y_TEST_PATH,delimiter=',',dtype=int)
x_test,x_train_all = x_test / 255.0, x_train_all /255.0
values=y_train_all[:5]
np.eye(10)[values]
y_train_all=np.eye(10)[y_train_all]
y_test=np.eye(10)[y_test]
x_val=x_train_all[:VALIDATION_SIZE]
y_val=y_train_all[:VALIDATION_SIZE]
x_train=x_train_all[VALIDATION_SIZE:]
y_train=y_train_all[VALIDATION_SIZE:]
x_train.shape
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()
X= tf.placeholder(tf.float32,shape=[None,TOTAL_INPUTS],name='X')
Y= tf.placeholder(tf.float32,shape=[None,NR_CLASSES],name='Labels')
nr_epochs = 50
n_hidden1= 512
n_hidden2= 64
learning_rate=0.001
with tf.name_scope('Hidden1_layer'): 
  # weights between input layer and 1st hidden layers like w11,w12 etc
  initial_w1=tf.truncated_normal(shape=[TOTAL_INPUTS,n_hidden1],stddev=0.1,seed=42)
  w1=tf.Variable(initial_value=initial_w1,name='Weight1') 
  # Declaring biases of nodes
  initial_b1=tf.constant(value=0.0,shape=[n_hidden1])
  b1=tf.Variable(initial_value=initial_b1,name='bias1')
  # input to the first hidden layer
  layer1_in=tf.matmul(X,w1)+b1
  # Output from the first layer
  layer1_out= tf.nn.relu(layer1_in)

  layer1_drop=tf.nn.dropout(layer1_out,keep_prob=0.8,name='Dropout_layer')

  tf.summary.histogram('Weights',w1)
  tf.summary.histogram('Biases',b1)
with tf.name_scope('Hidden2_layer'):
    # weights between 1st hidden layer and 2st hidden layers like w21,w22 etc
  initial_w2=tf.truncated_normal(shape=[n_hidden1,n_hidden2],stddev=0.1,seed=42)
  w2=tf.Variable(initial_value=initial_w2,name='Weight2') 
  # Declaring biases of nodes
  initial_b2=tf.constant(value=0.0,shape=[n_hidden2])
  b2=tf.Variable(initial_value=initial_b2,name='Bias2')
  # input to the second hidden layer
  layer2_in=tf.matmul(layer1_drop,w2)+b2
  # Output from the second layer
  layer2_out= tf.nn.relu(layer2_in)

  tf.summary.histogram('Weights',w2)
  tf.summary.histogram('Biases',b2)
with tf.name_scope('Output_layer'):
  # weights between 2ndt hidden layer and output layers like Wout1,Wout2 etc
  initial_w3=tf.truncated_normal(shape=[n_hidden2,NR_CLASSES],stddev=0.1,seed=42)
  w3=tf.Variable(initial_value=initial_w3,name='Weight3')
  # Declaring biases of nodes
  initial_b3=tf.constant(value=0.0,shape=[NR_CLASSES])
  b3=tf.Variable(initial_value=initial_b3,name='Bias3')
  # input to the output layer
  layer3_in=tf.matmul(layer2_out,w3)+b3
  # Output from the output layer
  output= tf.nn.softmax(layer3_in) 

  tf.summary.histogram('Weights',w3)
  tf.summary.histogram('Biases',b3)
folder_name=f'Model_1 at {strftime("%H:%M")}'
directory=os.path.join(LOGGING_PATH,folder_name)

try:
  os.makedirs(directory)
except OSError as exception:
  print(exception.strerror)
else:
  print('directories created successfully')
loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y,logits=output)) 
optimizer=tf.train.AdamOptimizer(learning_rate)
train_step=optimizer.minimize(loss)
correct_pred=tf.equal(tf.argmax(output,axis=1),tf.argmax(Y,axis=1)) 
accuracy=tf.reduce_mean(tf.cast(correct_pred,tf.float32))

tf.summary.scalar('accuracy',accuracy)
tf.summary.scalar('loss',loss)
x_image=tf.reshape(X,[-1,28,28,1])
tf.summary.image('image_input',x_image,max_outputs=4)
sess=tf.Session()
merged_summary=tf.compat.v1.summary.merge_all()
train_writer=tf.compat.v1.summary.FileWriter(directory + '/train')
train_writer.add_graph(sess.graph)
validation_writer=tf.compat.v1.summary.FileWriter(directory + '/validation')
init=tf.global_variables_initializer()
sess.run(init)
size_of_batch=1000
num_examples=y_train.shape[0]
nr_iterations=int(num_examples/size_of_batch)
index_in_epoch=0
def next_batch(batch_size,data,labels):
  global num_examples
  global index_in_epoch
  start=index_in_epoch
  index_in_epoch+=batch_size
  if index_in_epoch > num_examples:
    start = 0
    end=batch_size
  end=index_in_epoch
  return data[start:end] , labels[start:end]
for epoch in range(nr_epochs):
  for i in range(nr_iterations):
    batch_x,batch_y=next_batch(batch_size=size_of_batch,data=x_train,labels=y_train)
    feed_dictionary={X:batch_x,Y:batch_y}

    sess.run(train_step,feed_dict=feed_dictionary) 
  s,batch_accuracy=sess.run(fetches=[merged_summary,accuracy],feed_dict=feed_dictionary)
  train_writer.add_summary(s,epoch)
  
  print(f'Epoch {epoch} \t | Training accuracy = {batch_accuracy}')
  summary=sess.run(fetches=merged_summary,feed_dict={X:x_val,Y:y_val})
  validation_writer.add_summary(summary,epoch)
img=Image.open('test_img.png')
img
bw=img.convert('L')
img_array=np.invert(bw)
img_array.shape
test_img=img_array.ravel()
test_img.shape
prediction=sess.run(fetches=tf.argmax(output,axis=1),feed_dict={X:[test_img]})
prediction[0]
corr_pred=sess.run(fetches=correct_pred,feed_dict={X:x_test,Y:y_test})
corr_pred
type(corr_pred)
prediction_series = pd.Series(corr_pred)
prediction_series.value_counts()
from tensorflow.python.framework import ops
train_writer.close()
validation_writer.close()
sess.close()
ops.reset_default_graph()
